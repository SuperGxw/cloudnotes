import{_ as t,c as a,a as r,o as s}from"./app-DlVXqZPT.js";const n="/cloudnotes/assets/taxonomy-C2dvjhRY.png",l="/cloudnotes/assets/GNN_as_prefix-jYpvKSE8.png",i="/cloudnotes/assets/LLM_as_prefix-XWbd-6dV.png",o="/cloudnotes/assets/LLM-Graph_Intergration-DB3dwHNF.png",p="/cloudnotes/assets/LLM_only-CuLlpX0S.png",c={};function L(m,e){return s(),a("div",null,e[0]||(e[0]=[r('<div class="hint-container tip"><p class="hint-container-title">提示</p><p>根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容</p><p>阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考</p></div><div class="hint-container info"><p class="hint-container-title">信息</p><p>论文 <a href="https://arxiv.org/pdf/2405.08011v1" target="_blank" rel="noopener noreferrer">A Survey of Large Language Models for Graphs</a></p><p>相关 <a href="https://github.com/HKUDS/Awesome-LLM4Graph-Papers" target="_blank" rel="noopener noreferrer">https://github.com/HKUDS/Awesome-LLM4Graph-Papers</a></p><p>摘要:</p></div><p><img src="'+n+'" alt="alt text"></p><h3 id="gnns-as-prefix" tabindex="-1"><a class="header-anchor" href="#gnns-as-prefix"><span>GNNs as Prefix</span></a></h3><p>GNN 作为处理图数据的第一个组件，并为 LLM 提供结构感知 tokens (节点级，边级以及图级)进行推理。 在这种方式下， GNN 作为结构编码器，以增强 LLM 对图结构的理解，从而有利于下游任务。这种情况下，GNN 通常扮演 tokenizer 的角色，将图数据编码为具有丰富结构信息的图 token 序列，然后输入到 LLM 中与自然语言对齐。</p><p><img src="'+l+'" alt="alt text"></p><ul><li>Node-level Tokenization 图结构的每个节点都输入到 LLM 中，旨在使 LLM 理解细粒度的节点级结构信息并区分关系。</li><li>Graph-level Tokenzation 使用特定的池化方法将图压缩为固定长度的 token 序列，旨在捕获图结构的高级全局语义信息。</li></ul><h3 id="llms-as-prefix" tabindex="-1"><a class="header-anchor" href="#llms-as-prefix"><span>LLMs as Prefix</span></a></h3><p>LLM 首先使用文本信息处理图数据，然后提供节点嵌入或生成标签来改进图神经网络的训练。 利用大型语言模型产生的信息来改进图神经网络的训练。此信息包括从大型语言模型派生的文本内容、标签或嵌入。</p><p><img src="'+i+'" alt="alt text"></p><ul><li>Embeddings from LLMs for GNNs 使用大型语言模型为图神经网络生成的嵌入。</li><li>Labels from LLMs for GNNs 集成大型语言模型为图神经网络生成的标签。此上下文中的监督标签不限于分类任务中的分类标签，而是可以采用嵌入、图等多种形式。从LLM 生成的信息不被用作 GNN 的输入，而是形成监督信号进行更好的优化，这使得gnn能够在各种与图相关的任务上实现更高的性能。</li></ul><h3 id="llms-graphs-integration" tabindex="-1"><a class="header-anchor" href="#llms-graphs-integration"><span>LLMs-Graphs Integration</span></a></h3><p>LLM 实现与图数据的更高层次的集成，如与 GNN 的融合训练或对齐，并构建基于 LLM 的代理与图信息交互。</p><p><img src="'+o+'" alt="alt text"></p><ul><li>Alignment between GNNs and LLMs GNN 和 LLM 旨在处理不同形式的数据，GNN 专注于结构化数据，LLM 专注于文本数据。这导致两个模型的特征空间不同。为了解决这个问题并使两种模式的数据更有利于 GNN 和 LLM 的学习，通常使用对比学习或期望最大化 (EM) 迭代训练等技术来对齐两个模型的特征空间。使得能够更好地对图和文本信息进行建模，从而提高各种任务的性能。</li><li>Fusion Training of GNNs and LLMs 尽管 GNN 和 LLM 的表示之间的对齐实现了两个模型的协同优化和嵌入级对齐，但它们在推理过程中保持不变。为了实现 LLM 和 GNN 之间的更高层次的集成，一些工作专注于设计模块的体系结构的更深层次的融合，如llm中的变压器层和 GNN 中的图神经网络。协同训练 GNN 和 LLM 可以为图任务中的两个模块带来双赢的双向好处。</li><li>LLMs Agent for Graphs 一个新兴的研究方向是基于 LLM 构建自主代理，以解决人类给定的或与研究相关的任务。通常，代理由内存模块、感知模块和动作模块组成，以实现观察、记忆召回和动作循环以解决给定任务。在图域中，基于llm的代理可以直接与图数据交互，以执行节点分类和链接预测等任务。</li></ul><h3 id="llms-only" tabindex="-1"><a class="header-anchor" href="#llms-only"><span>LLMs-Only</span></a></h3><p>设计实用的提示方法，将图结构数据接到 LLM 单词序列中进行推断，有些方法也包含多模态 tokens。 允许 LLM 直接接受图结构信息，理解它，并与这些信息一起对各种下游任务进行推理。</p><p><img src="'+p+'" alt="alt text"></p><ul><li>Tuning-free 旨在设计 LLM 可以理解表达图的提示，直接提示预先训练的 LLM 来执行面向图的任务。</li><li>Tuning-required 侧重于以特定方式将图转换为序列，并使用微调方法将图 token 序列和自然语言 token 序列对齐。</li></ul>',19)]))}const h=t(c,[["render",L],["__file","index.html.vue"]]),d=JSON.parse('{"path":"/article/wze856pj/","title":"图大语言模型综述","lang":"zh-CN","frontmatter":{"title":"图大语言模型综述","tags":["图学习","大语言模型","综述"],"createTime":"2024/10/29 22:33:58","permalink":"/article/wze856pj/","description":"提示 根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容 阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考 信息 论文 A Survey of Large Language Models for Graphs 相关 https://github.com/HKUDS/Awesome-LLM4Graph-Papers...","head":[["meta",{"property":"og:url","content":"https://supergxw.github.io/cloudnotes/article/wze856pj/"}],["meta",{"property":"og:site_name","content":"Super_GXW"}],["meta",{"property":"og:title","content":"图大语言模型综述"}],["meta",{"property":"og:description","content":"提示 根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容 阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考 信息 论文 A Survey of Large Language Models for Graphs 相关 https://github.com/HKUDS/Awesome-LLM4Graph-Papers..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-02T11:34:37.000Z"}],["meta",{"property":"article:tag","content":"图学习"}],["meta",{"property":"article:tag","content":"大语言模型"}],["meta",{"property":"article:tag","content":"综述"}],["meta",{"property":"article:modified_time","content":"2025-06-02T11:34:37.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"图大语言模型综述\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-02T11:34:37.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":3,"title":"GNNs as Prefix","slug":"gnns-as-prefix","link":"#gnns-as-prefix","children":[]},{"level":3,"title":"LLMs as Prefix","slug":"llms-as-prefix","link":"#llms-as-prefix","children":[]},{"level":3,"title":"LLMs-Graphs Integration","slug":"llms-graphs-integration","link":"#llms-graphs-integration","children":[]},{"level":3,"title":"LLMs-Only","slug":"llms-only","link":"#llms-only","children":[]}],"readingTime":{"minutes":3.83,"words":1148},"git":{"createdTime":1748864077000,"updatedTime":1748864077000,"contributors":[{"name":"SuperGxw","email":"627215564@qq.com","commits":1}]},"autoDesc":true,"filePathRelative":"graphllm/survey.md","categoryList":[{"id":"5c879b","sort":10001,"name":"graphllm"}]}');export{h as comp,d as data};
