import{_ as a,c as n,a as t,o as e}from"./app-DlVXqZPT.js";const p="/cloudnotes/assets/anygpt_1-BnB0PxT2.png",i="/cloudnotes/assets/anygpt_2-DDThvxuY.png",l="/cloudnotes/assets/anygpt_3-CsQhUBpc.png",o="/cloudnotes/assets/anygpt_4-CgiLbU6K.png",m={};function r(c,s){return e(),n("div",null,s[0]||(s[0]=[t('<div class="hint-container tip"><p class="hint-container-title">提示</p><p>根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容</p><p>阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考</p></div><div class="hint-container info"><p class="hint-container-title">信息</p><p>论文 <a href="https://arxiv.org/pdf/2402.12226" target="_blank" rel="noopener noreferrer">AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</a></p><p>主页 <a href="https://junzhan2000.github.io/AnyGPT.github.io/" target="_blank" rel="noopener noreferrer">https://junzhan2000.github.io/AnyGPT.github.io/</a></p><p>代码 <a href="https://github.com/OpenMOSS/AnyGPT" target="_blank" rel="noopener noreferrer">https://github.com/OpenMOSS/AnyGPT</a></p><p>摘要: 本文主要介绍了一种名为AnyGPT的多模态语言模型，一种任意到任意模态的语言模型。它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。 AnyGPT 可以稳定地训练，无需对当前的大语言模型（LLM）架构或训练范式进行任何改变。</p></div><h2 id="论文贡献" tabindex="-1"><a class="header-anchor" href="#论文贡献"><span>论文贡献</span></a></h2><blockquote><p>1）提出了一种基于 token 的任意到任意模型的多模态语言模型AnyGPT，可以理解和生成各种模态，包括语音、文本、图像和音乐。</p></blockquote><blockquote><p>2）使用生成模型开发了一个管道来构建了一个包含108k个具有交错多模态元素的多轮对话的数据集AnyInstruct-108k。</p></blockquote><blockquote><p>3）证明了离散表示可以有效地统一语言模型中的多种模态。</p></blockquote><h2 id="模型架构" tabindex="-1"><a class="header-anchor" href="#模型架构"><span>模型架构</span></a></h2><p>AnyGPT主要包含三个组成部分：多模态 tokenizer、作为骨干网络的多模态语言模型 和多模态 de-tokenzier。tokenizer 将连续的非文本模态转换为离散 tokens，随后排列成多模态交错序列。然后语言模型使用下一个 token 预测目标进行训练。在推理阶段通过 de-tokenizer 将多模态 tokens 解码回其原始表征。 <img src="'+p+'" alt="alt text"></p><h3 id="tokenization" tabindex="-1"><a class="header-anchor" href="#tokenization"><span>Tokenization</span></a></h3><h4 id="图像tokenizer" tabindex="-1"><a class="header-anchor" href="#图像tokenizer"><span>图像tokenizer</span></a></h4><p><img src="'+i+'" alt="alt text"></p><p>图像 tokenizer 使用的是SEED tokenizer（Tencent，2023），主要包含几个组件：Vit encoder、Causal Q-Former、VQ Codebook、MLP以及UNet decoder。以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\\times 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">224</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">224</span></span></span></span>RGB 的图像作为输入，ViT encoder 将其编码为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16\\times 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span>的 patches，然后 Causal Q-Former 将这些 patches 特征转换成 32 维的 causal embeddings，接着用具有 8192 项的 codebook 将 causal embeddings 离散化为一系列的矢量 codes，MLP用于将视觉 codes 解码为生成嵌入，该嵌入与预先训练的 unCLIPSD 的潜在空间对齐。最后，UNet 解码器用于将生成嵌入恢复为原始图像。</p><h4 id="语音tokenizer" tabindex="-1"><a class="header-anchor" href="#语音tokenizer"><span>语音tokenizer</span></a></h4><p><img src="'+l+'" alt="alt text"> 语音 tokenizer 使用的是 SpeechTokenizer（Fudan，2023）。采用具有残差矢量量化 (RVQ) 的 encoder-decoder 架构。SpeechTokenizer 使用八个分层量化器将单通道音频序列压缩为离散矩阵，每个量化器有 1024 个条目，并实现了 50 Hz 的帧速率。第一个量化器层捕获语义内容，而第 2 层到第 8 层编码副语言细节。因此，将 10 秒的音频转换为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500</mn><mo>×</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">500\\times 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">500</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8</span></span></span></span>矩阵，分为语义和声学 tokens。</p><h4 id="音乐tokenizer" tabindex="-1"><a class="header-anchor" href="#音乐tokenizer"><span>音乐tokenizer</span></a></h4><p><img src="'+o+'" alt="alt text"></p><p>音乐 tokenizer 使用的是 Encodec（Meta，2022）。使用残差矢量量化（RVQ）量化的潜在空间的卷积auto-encoder 作为音乐 tokenizer。使用了在 20k 条音乐轨道上预训练的 Encodec1 的可用现成变体。该变体处理 32 kHz 单音音频，并实现了 50 Hz 的帧速率。生成的嵌入使用具有四个量化器的 RVQ 进行量化，每个量化器的 codebook 大小为 2048，得到组合音乐词汇量为 8192。</p><h3 id="语言模型-backbone" tabindex="-1"><a class="header-anchor" href="#语言模型-backbone"><span>语言模型 Backbone</span></a></h3><h4 id="扩展词汇表" tabindex="-1"><a class="header-anchor" href="#扩展词汇表"><span>扩展词汇表</span></a></h4><p>为了将多模态离散表示合并到预训练的 LLM 中，使用新的模态特定 tokens 扩展词汇表，从而扩展相应的嵌入和预测层，新合并的参数是随机初始化的。来自所有模态的 tokens 组合起来形成一个新的词汇表，其中每个模态都在语言模型中进行训练，以在共享表示空间中对齐。这个增量词汇表的大小用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>表示，是所有模态的词汇量的总和，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>V</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">V=\\sum_{i=1}^{n}V_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">V_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>个模态的词汇量。</p><h4 id="统一多模态语言模型" tabindex="-1"><a class="header-anchor" href="#统一多模态语言模型"><span>统一多模态语言模型</span></a></h4><p>配备特定于模态的 tokenizer，将多模态数据压缩为离散 token 序列，可以使用下一个 token 预测损失由语言模型训练。这自然使核心LLM能够以自回归的方式统一感知、理解、推理和生成等任务。这里使用了LLaMA-2 7B 作为主干。</p><h4 id="多模态生成" tabindex="-1"><a class="header-anchor" href="#多模态生成"><span>多模态生成</span></a></h4><p>高质量多模态数据的生成，包括高清图像和高保真音频，提出了一个重大挑战。这些数据通常需要大量位才能进行准确的表示，导致长序列对语言模型特别苛刻，因为计算复杂度随着序列长度呈指数增长。 为了解决这个问题，本文采用两阶段框架进行高保真生成，包括语义信息建模和感知信息建模。首先，语言模型的任务是在语义级别生成经过融合和对齐的内容。然后，非自回归模型将多模态语义标记转换为感知级别的高保真多模态内容，在性能和效率之间取得平衡。 具体来说，使用与扩散潜在空间对齐的 SEED tokens 进行视觉语言建模。语义级 SEED tokens 通过扩散模型解码为高质量图像，以其优越的生成能力而闻名。对于语音，利用 SoundStorm (google，2023)，这是一种非自回归掩码语言模型，经过训练可以从语义标记中生成 SpeechTokenizer 的声学标记。本文训练了一个 Soundstorm 变体，它是在多语言 LibriSpeech（MLS）数据集上使用 SpeechTokenizer 训练的。随后，SpeechTokenizer 的解码器将所有语音 tokens 转换为原始音频数据。这种方法使 AnyGPT 使用 3 秒的语音提示复制任何说话者的语音，同时显着减少 LLM 的语音序列的长度。对于音乐，使用 Encodec 标记过滤掉人类感知之外的高频细节，然后使用 Encodec 解码器将这些标记重建为高保真音频数据。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>在这项工作中，提出了 AnyGPT，这是一种任意到任意多模态语言模型，它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。离散多模态表示促进了新模态的无缝集成，不需要改变现有的LLM架构或训练范式。为了使模型能够处理多模态输入和输出的任意组合，合成了第一个大规模任意到任意多模态指令数据集 AnyInstruct-108k，该数据集由复杂交织各种模态的多轮对话组成。实验结果表明，AnyGPT在各种跨模态任务中取得了很好的结果，并证明了离散表示可以有效地方便地统一一个统一的大型语言模型中的多个模态。</p><h3 id="限制和未来工作" tabindex="-1"><a class="header-anchor" href="#限制和未来工作"><span>限制和未来工作：</span></a></h3><p>1）Benchmark：任意到任意多模态大型语言模型 (LLM) 的领域都是新兴的研究领域。然而，缺乏专门的基准来评估模型跨多个维度的能力，以及减轻潜在风险，提出了一个相当大的挑战。因此，综合基准的开发势必行。 2）增强 LLMs：虽然具有离散表示的多模态 LLM 可以稳定地训练，但与单模态训练相比，可以观察到更高的损失，阻碍了每个模态的最佳性能。改进多模态融合的潜在策略可能涉及缩放 LLM 和 tokenizers 或采用混合专家 (MOE) 架构来更好地管理不同的数据并优化性能。 3）更好的 tokenizer：在采用离散表示的多模态 LLM 中，tokenizer 的质量为模型的理解和生成潜力设置了上限。增强 tokenizer 可以从各种角度进行处理，包括采用优越的 codebook 训练方法、开发更有凝聚力的多模态表示在各种模态中的应用。 4）更长的上下文：多模态内容，如图像和音频，通常跨越广泛的序列。例如，AnyGPT 将音乐建模限制为 5 秒，极大地限制了其音频输出的实际有用性。此外，对于任意到任意多模态对话，扩展上下文允许更多的会话交换，从而丰富交互的深度和复杂性。</p>',28)]))}const d=a(m,[["render",r],["__file","index.html.vue"]]),g=JSON.parse('{"path":"/article/b02a9k62/","title":"AnyGPT：离散序列建模统一多模态LLM","lang":"zh-CN","frontmatter":{"title":"AnyGPT：离散序列建模统一多模态LLM","tags":["多模态","大语言模型"],"createTime":"2024/10/13 17:25:03","permalink":"/article/b02a9k62/","description":"提示 根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容 阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考 信息 论文 AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling 主页 https://junzhan2000.github.io/A...","head":[["meta",{"property":"og:url","content":"https://supergxw.github.io/cloudnotes/article/b02a9k62/"}],["meta",{"property":"og:site_name","content":"Super_GXW"}],["meta",{"property":"og:title","content":"AnyGPT：离散序列建模统一多模态LLM"}],["meta",{"property":"og:description","content":"提示 根据遗忘曲线：如果没有记录和回顾，6天后便会忘记75%的内容 阅读笔记正是帮助你记录和回顾的工具，不必拘泥于形式，其核心是：记录、翻看、思考 信息 论文 AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling 主页 https://junzhan2000.github.io/A..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-02T11:34:37.000Z"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:tag","content":"大语言模型"}],["meta",{"property":"article:modified_time","content":"2025-06-02T11:34:37.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"AnyGPT：离散序列建模统一多模态LLM\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-02T11:34:37.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"论文贡献","slug":"论文贡献","link":"#论文贡献","children":[]},{"level":2,"title":"模型架构","slug":"模型架构","link":"#模型架构","children":[{"level":3,"title":"Tokenization","slug":"tokenization","link":"#tokenization","children":[]},{"level":3,"title":"语言模型 Backbone","slug":"语言模型-backbone","link":"#语言模型-backbone","children":[]}]},{"level":2,"title":"总结","slug":"总结","link":"#总结","children":[{"level":3,"title":"限制和未来工作：","slug":"限制和未来工作","link":"#限制和未来工作","children":[]}]}],"readingTime":{"minutes":7.15,"words":2144},"git":{"createdTime":1748864077000,"updatedTime":1748864077000,"contributors":[{"name":"SuperGxw","email":"627215564@qq.com","commits":1}]},"autoDesc":true,"filePathRelative":"mllm/anygpt.md","categoryList":[{"id":"2a0b20","sort":10002,"name":"mllm"}]}');export{d as comp,g as data};
